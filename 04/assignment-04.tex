\title{CS 726 Assignment 4}
\author{Ruochen Lin}
\documentclass[11pt]{article}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{commath}
\begin{document}
\maketitle
\section{}
Given $d_k = -\nabla f(x_k)$ and $\alpha \in (0,\frac1L)$, we have $$x_{k+1} = x_k - \alpha \nabla f(x_k).$$ Also, equation (3.8) in the manuscript reads $$f(x + \alpha d)\leqslant f(x) +\alpha \nabla f(x)^Td + \alpha^2\frac L2\norm{d}^2;$$
by plugging in $d = -\nabla f(x_k)$ we have 
\begin{equation}\begin{split} 
f(x_{k+1}) &\leqslant f(x_k) - \alpha \norm{\nabla f(x_k)}^2 + \frac{\alpha^2L}2\norm{\nabla f(x_k)}^2 \\
&= f(x_k) - \frac{\alpha(2-\alpha L)}2\norm{\nabla f(x_k)}^2.
\end{split}\end{equation} 
\subsection{General case}
Rearrange equation (1) we have:
\begin{equation}\begin{split}\norm{\nabla f(x_k)}^2\leqslant \frac2{\alpha(2-\alpha L)}[f(x_k) - f(x_{k+1})]. \end{split}\end{equation} 
Sum both sides of equation (2) from 0 to n, we have
\begin{equation}\begin{split}
\sum_{k=0}^{n} \norm{\nabla f(x_k)}^2 &\leqslant \frac2{\alpha(2-\alpha L)}[f(x_0) - f(x_{n+1})]\\
&\leqslant \frac2{\alpha(2-\alpha L)} [f(x_0) - f(x^*)] \\
\Rightarrow \min_{k = 0,\cdots, n}\norm{\nabla f(x_k)} &\leqslant \sqrt{\frac{2(f(x_0) - f(x^*))}{\alpha(n+1)(2-\alpha L)}},
\end{split}\end{equation}  
thus preserving the $\frac1{\sqrt{n}}$ convergence rate; note that if we plug $\alpha = \frac1L$ into the inequality above, we recover the results for steepest descent in the manuscript.

\subsection{Convex case}
If $f(x)$ is convex, then we have 
\begin{equation}\begin{split}
f(x^*)&\geqslant f(x_k)+\nabla f(x_k)^T(x^*-x_k) \\ 
\Rightarrow f(x_k) &\leqslant f(x^*) - \nabla f(x_k)^T (x^* - x_k).
\end{split}\nonumber\end{equation} 
Plug this into equation (1) we have 
\begin{equation}\begin{split} 
f(x_{k+1}) &\leqslant f(x^*) - \nabla f(x_k)^T(x^*-x_k) - \frac{\alpha(2-\alpha L)}2\norm{\nabla f(x_k)}^2 \\
&=f(x^*) - \frac{\alpha(2-\alpha L)}2\nabla f(x_k)^T[\frac2{\alpha(2-\alpha L)}(x^*-x_k)+\nabla f(x_k)] \\
&=f(x^*) - \frac{\alpha(2-\alpha L)}2\Big(\norm{\nabla f(x_k) +\frac{x^*-x_k}{\alpha(2-\alpha L)}}^2 - \norm{\frac{x^*-x_k}{\alpha(2-\alpha L)}}^2\Big) \\
&=f(x^*) - \frac1{2\alpha(2-\alpha L)}\Big( \norm{\alpha(2-\alpha L)\nabla f(x_k) + x^* - x_k}^2 - \norm{x^* - x_k}^2  \Big) \\
&\leqslant f(x^*) - \frac1{2\alpha(2-\alpha L)}\Big(\norm{\alpha\nabla f(x_k) + x^* - x_k}^2 - \norm{x^* - x_k}^2 \Big)\\
&= f(x^*) - \frac1{2\alpha(2-\alpha L)}\Big( \norm{x^*-x_{k+1}}^2 - \norm{x^* - x_k}^2  \Big)\\
\end{split}\nonumber\end{equation} 
\begin{equation}\begin{split} 
\Longrightarrow\,\, f(x_{k+1}) - f(x^*) \leqslant  \frac1{2\alpha(2-\alpha L)}\Big( \norm{x^*-x_k}^2 - \norm{x^* - x_{k+1}}^2  \Big)
\end{split}\end{equation} 
Sum inequality (4) from $k=0$ to $n-1$, we have
\begin{equation}\begin{split}
\sum_{k=0}^{n-1}(f(x_{k+1}) - f(x^*)) &\leqslant  \frac1{2\alpha(2-\alpha L)}(\norm{x_0-x^*}^2 - \norm{x_n-x^*}^2) \\
&\leqslant  \frac1{2\alpha(2-\alpha L)} \norm{x_0 - x^*}^2
\end{split}\end{equation}
Because $f(x_k)$ is decreasing, 
$$f(x_{n}) - f(x^*)\leqslant \frac1{2n\alpha(2-\alpha L)}\norm{x_0 - x^*}^2 = o(\frac1n).$$

\subsection{}
%TODO: analyse strongly convex case

\section{}
\subsection{}
There are infinite solutions, because for underdetermined (\textit{i.e.} $n<d$) case like this, there is either no solution or infinite solutions.

\subsection{}
Instead of minimizing the function $f_0(x) = \frac1n\norm{Ax-b}^2$, we opt to minimize the function $f(x) = \norm{Ax-b}^2$ to get rid of the cumbersome coefficient $\frac1n$. Since they only differ in a factor of constant, they'll have the same convergence peoperties, the same minimizer, and even the same minimum $0$; only that the error in $f_0$ is $10$ times smaller than that of $f$:
$$f_0(x) \leqslant \epsilon \,\,\Leftrightarrow f(x) \leqslant n\epsilon.$$
We first note that $f(x)$ (and $f_0(x)$, of course) is convex, but not strongly convex. To prove this, we first write $f(x)$ in the form of a quadratic function:
\begin{equation}\begin{split}
f(x) &= x^TA^TAx -2b^TAx+b^Tb, \\
\nabla f(x) &= 2 A^TAx-2A^Tb = 2A^T(Ax-b).
\end{split}\nonumber\end{equation} 
$f(x)$ is convex because 
\begin{equation}\begin{split} 
&f(y) - f(x) -\nabla f(x)^T(y-x) \\
=& y^TA^TAy - 2b^TAy - x^TA^TAx + 2b^TAx \\
&- 2x^TA^TA(y-x) + 2b^TA(y-x)\\
=& y^TA^TAy -2x^TA^TAy + x^TA^TAx \\
=&\norm{Ax-Ay}^2 \geqslant 0 \\
\end{split}\nonumber\end{equation} 
$$\Longrightarrow \,\, f(y) \geqslant f(x) +\nabla f(x)(y-x);$$
it's not strongly convex because $A^TA$ is singular and must have $0$ as at least one of its eigenvalues: $\text{rank}(A^TA) = \text{rank}($A$) = n < d$, but $A^TA$ is a $d\times d$ matrix; in other words, $A^TA$ is positive semidefinite.\\[0.3cm]
In order to make $f(x)$ fit into our analysis in class, we define $A' = 2A^TA$, so that $f(x) = \frac12 x^TA'x-2b^TAx+b^Tb$. Suppose the largest eigenvalue of $A'$ is $L$, and $x^*$ is a minimizer of $f$, then
\begin{equation}\begin{split} 
f(x_K) - f(x^*) = f(x_K) &\leqslant \frac L{2K}\norm{x_0-x^*}^2\\
&=\frac L{2K}\norm{x^*}^2;
\end{split}\nonumber\end{equation}
if we require the error within $K$ steps in $f(x)$ to be smaller than $n\epsilon$, then 
$$K \leqslant \frac L{2n\epsilon}\norm{x^*}^2.$$
Here are some notes:
\begin{itemize}
\item Since $f(x)$ has infinite minimizers, our iteration does not necessarily lead to $x_k\to x^*$; however, $\norm{x^*}$ can still be used to bound our error; in fact, the error can be bounded by $\norm{x^{\dagger}}=\min_{\{x\,:\,Ax=b\}}\norm{x}$, with $x^{\dagger}$ being the minimizer of $f(x)$ that has the smallest Euclidean distance from origin.
\item The relationship between the spectra of $A'$ and $A^TA$ is the following: $\lambda_i(A') = 2\lambda_i(A^TA)$; thus if we define $L'$ as the largest eigenvalue of $A^TA$, then we should replace the $L$s in our inequality with $2L'$.
\item Writing $f(x)$ in its quadratic form is only of conceptual use to us; we would never want of calculate $A^TA$, which has the complexity of about $O(nd^2)$, in practice. Evaluating $f(x) = \norm{Ax-b}^2$ and $\nabla f(x) = 2A^T(Ax-b)$ each costs us $O(nd)$, and doing a exact line serch would have similar time complexity, if we alway evaluate expressions like $x^TA^TAx$ as $(Ax)^T(Ax)$. 
\item If $n$ is small compared to $d$, then we probably can afford to evaluate the matrix product $AA^T$, which costs $O(n^2d)$, at the beginning of the program. By doing so, we can transform the problem into one with much nicer properties: Minimizing $g(t) = \norm{AA^Tt-b}^2$. Because $AA^T$ is a $n\times n$ matrix with rank $n$, it is now invertible and thus is positive definite, (instead of being positive semi-definite, as we've seen above,) making $g(t)$ strictly convex. Now that $g(t)$ is a strictly convex function, we can yield much faster convergence with descent methods: given that the condition number of $A^TA$ is $\kappa$, after $K$ iterations we would have $$g(t_K)\leqslant (1-\frac1{\kappa})^K\norm{b}^2$$
$$\Rightarrow\,\,k \geqslant \frac{\ln\frac{n\epsilon}{\norm{b}^2}}{\ln(1-\frac1{\kappa})},$$
if we want the error in $g(t)$ to be smaller than $n\epsilon$. Finally, we can get the corresponding minimizer in $x$-space with 
$$x = A^Tt.$$ The solution we get from this algorithm is also the $x^{\dagger}$ we mentioned above, namely the solution that's closest to origin. 
\end{itemize}

\subsection{}
\begin{equation}\begin{split} 
l_{\mu}(x) &= \frac1n\norm{Ax-b}^2+\mu\norm{x}^2,\\
\nabla l_{\mu}(x) &= \frac2n A^T(Ax-b) + 2\mu x = (\frac2n A^TA + \mu I)x - \frac2n A^Tb.
\end{split}\nonumber\end{equation}	
At the minimizer of $l_{\mu}$, we have 
\begin{equation}\begin{split}
\nabla l_{\mu}(x^{(\mu)}) &= 0 \\
\Rightarrow\,\,\big( \frac2n A^TA  +2\mu I \big)&x^{(\mu)} = \frac{2A^Tb}n,\\
x^{(\mu)} = \big(A^TA+&n\mu I  \big)^{-1}A^Tb.
\end{split}\nonumber\end{equation} 
Here $A^TA+n\mu I$ is invertible because $A^TA$ is positive semidefinite, as we've shown above, and $n\mu I$ is positive definite, making the sum positive definite and thus invertible.

\subsection{}
\begin{equation}\begin{split} 
l_{\mu}(x) & = \frac2n\norm{Ax-b}^2+\mu\norm{x}^2\\
&=x^T(\frac1n A^TA + \mu I)x-\frac2n b^TAx+\norm{b}^2.
\end{split}\nonumber\end{equation} 
If we define $\tilde A = \frac1nA^TA+\mu I$, and its condition number $\tilde \kappa$, then
\begin{equation}\begin{split} 
l_{\mu}(x_k) - l_{\mu}(x^{\mu}) &\leqslant (1-\frac1{\tilde \kappa})^k(l_{\mu}(x_0)-l_{\mu}(x^{(\mu)}))\\
&=(1-\frac1{\tilde \kappa})^k(\frac{\norm{b}^2}n-l_{\mu}(x^{(\mu)})).
\end{split}\nonumber\end{equation} 
If we desire the left-hand side of the inequality above to be no larger than $\epsilon$ after $K$ steps, then 
\begin{equation}\begin{split} 
(1-\frac1{\tilde\kappa})^K(&\frac{\norm{b}^2}n-l_{\mu}(x^{(\mu)}))\leqslant \epsilon\\
K\ln(1-\frac1{\tilde\kappa})&\leqslant \ln\frac{\epsilon}{\frac{\norm{b}^2}n-l_{\mu}(x^{(\mu)})},\\
K&\geqslant\frac{\ln\frac{\epsilon}{\frac{\norm{b}^2}n-l_{\mu}(x^{(\mu)})}}{\ln(1-\frac1{\tilde\kappa})}.
\end{split}\nonumber\end{equation} 

\subsection{}
\begin{equation}\begin{split}
\frac1n\norm{A\hat x-b}^2 &= l_{\mu}(\hat x)-\mu\norm{\hat x}^2 \\
&\leqslant \epsilon+l_{\mu}(x^{(\mu)})- \mu\norm{\hat x}^2;
\end{split}\nonumber\end{equation}
plug in the expression for $x^{(\mu)}$, we have
\begin{equation}\begin{split} 
\frac1n\norm{A\hat x - b}^2 &\leqslant \epsilon +\frac1n \norm{A(A^TA+n\mu I)^{-1}A^Tb-b}^2+\mu\norm{(A^TA+n\mu I)^{-1}A^Tb}^2-\mu\norm{\hat x}^2\\
&\leq \epsilon +\frac1n \norm{A(A^TA+n\mu I)^{-1}A^Tb-b}^2+\mu\norm{(A^TA+n\mu I)^{-1}A^Tb}^2.
\end{split}\nonumber\end{equation} 

% TODO: finish problem 2

\section{}
Given symmetric positive definite matrix $A$, if for a set of vectors $\{p_k\}$ we have 
$$p_i^TAp_j=0,\,\,\text{if}\,\,i\neq j,$$
then we define
$$P = \begin{bmatrix} p_0, & p_1, & \dots ,& p_l\end{bmatrix} $$
and
$$\Sigma = P^TAP = \begin{bmatrix} \sigma_0 \\ & \sigma_1 \\ & & \ddots \\ & & & \sigma_l \end{bmatrix}\succ0, $$
in which $\sigma_i = p_i^TAp_i > 0$ because $A$ is positive definite.\\[0.4cm]
We can prove the theorem by contradiction: if $\{p_k\}$ is linearly dependent, \textit{i.e.} $\exists x \neq 0$ such that $Px = \sum_{i=0}^lx_ip_i=0$, then, on one hand, 
\begin{equation} 
x^T\Sigma x = x^TP^TAPx = (Px)^TA(Px) = 0;
\nonumber\end{equation} 
on the other, since $\Sigma$ is positive definite, 
$$x^T\Sigma x > 0,$$
which contradicts the equation above. Hence we've shown that $\{p_k\}$ can only be linearly independent if they're conjugate with respect to symmetric possitive definte matrix $A$.

\section{}
\textbf{Lemma}: if $A$ is symmetric, then any polynomial of $A$ is also symmetric.\\[0.4cm]
\textit{Proof}: 
Suppose $A$ is an $n\times n$ symmetric matrix, then
\begin{equation}\begin{split}
(A^k)_{ij} &=\sum_{x_1=1}^n \sum_{x_2=1}^n\cdots\sum_{x_{k-1}=1}^nA_{ix_1}A_{x_1x_2}\cdots A_{x_{k-1}j}\\
&=\sum_{x_1=1}^n \sum_{x_2=1}^n\cdots\sum_{x_{k-1}=1}^nA_{x_1i}A_{x_2x_1}\cdots A_{jx_{k-1}}\\
&=(A^k)_{ji}.
\end{split}\nonumber\end{equation} 
And clearly the sum of symmetric matrices is also symmetric; and a polynomial of $A$ to the $k$th power is a sum of $k+1$ symmetric matrices, which is symmetric, and thus $[I+P_k(A)A]^T = [I + P_k(A)A].$\\[0,4cm]
With the result from the textbook that $P_k(A)v_i=P_k(\lambda_i)v_i$, we have:
\begin{equation}\begin{split} 
[I+P_k(A)A]^TA[I+P_k(A)A]v_i &= [I+P_k(A)A]A[v_i+P_k(A)Av_i]\\
&=[I+P_k(A)A]A[(1+\lambda_iP_k(\lambda_i))v_i] \\
&=(1+\lambda_iP_k(\lambda_i))[I+P_k(A)A]Av_i \\
&=\lambda_i(1+\lambda_iP_k(\lambda_i))[I+P_k(A)A]v_i\\
&=\lambda_i(1+\lambda_iP_k(\lambda_i))^2v_i.
\end{split}\nonumber\end{equation} 
Hence $(v_i,\lambda_i(1+\lambda_iP_k(\lambda_i))^2)$ is an eigenpair of $[I+P_k(A)A]^TA[I+P_k(A)A]$, given that $(v_i,\lambda_i)$ is one for $A$.

\section{}
For the given problem, the solution is 
$$x^* = \begin{bmatrix}1-\frac1{n+1} \\ 1-\frac2{n+1} \\ \vdots \\ 1-\frac n{n+1}\end{bmatrix},$$
and $x_k$ can have non-zero entries only in the first $k$ spots, given the starting point $x_0 = 0$.
% TODO: prove the statement above.
Thus
\begin{equation}\begin{split} 
\norm{x_0-x^*}^2 &= \norm{x^*}^2 = \sum_{j=1}^n\big(1 - \frac j{n+1} \big)^2 \\ 
&= \sum_{j=1}^n \big( 1 - \frac{2j}{n+1} + \frac{j^2}{(n+1)^2} \big) \\
&= n - \frac2{n+1}\sum_{j=1}^n j + \frac1{(n+1)^2}\sum_{j=1}^n j^2 \\
&= n - \frac2{n+1}\cdot \frac{n(n+1)}2 + \frac1{(n+1)^2}\cdot \frac{n(n+1)(2n+1)}6 \\
&= n - n + \frac{2n+1}{n+1}\cdot\frac{n}6 \\
&\leqslant \frac{2n}6 = \frac{n}3
\end{split}\nonumber\end{equation}  
and
\begin{equation}\begin{split} 
\norm{x_k-x^*}^2 &\geqslant \norm{\begin{bmatrix}0, & \cdots, & 0, & 1 - \frac{k+1}{n+1}, & \cdots, 1-\frac n{n+1} \end{bmatrix}^T } \\
&= \sum_{j=k+1}^n \big(1-\frac j{n+1} \big)^2 = \frac{\sum_{j=k+1}^n(n+1-j)^n}{(n+1)^2} \\
&= \frac1{(n+1)^2}\big((n-k)^2+(n-k-1)^2 + \cdots + 2^2 +1^2 \big)\\
&= \frac1{(n+1)^2}\cdot \frac{(n-k)(n-k+1)(2n-2k+1)}6 \\
&\geqslant \frac1{(n+1)^2}\cdot\frac{(n-k)(n-k)(2n-2k)}6 \\
&= \frac1{(n+1)^2}\cdot\frac{(n-k)^3}3 = \frac{(n-k)^3}{3(n+1)^2} \\
&\geqslant \frac{(n-k)^3}{n(n+1)^2}\norm{x_0-x^*}^2,
\end{split}\nonumber\end{equation}
in the last step of which we invoked the inequality we proved for $\norm{x_0-x^*}^2$.
\end{document}
